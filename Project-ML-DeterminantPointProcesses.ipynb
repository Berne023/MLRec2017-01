{
 "metadata": {
  "name": "",
  "signature": "sha256:cbefce0139d449c22f1c407f4b0710b1b49896b9182995a873163b7ec7c3e653"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Low rank factorization of determinant point processes for recommendation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.model_selection import train_test_split\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## We use the musical instrument dataset from Amazon"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.read_csv(\"ratings_Musical_Instruments.csv\", sep=\",\", header=None, names=[\"user\", \"item\",\"rating\",\"timestamp\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "del data['timestamp']\n",
      "data.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>user</th>\n",
        "      <th>item</th>\n",
        "      <th>rating</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>A1YS9MDZP93857</td>\n",
        "      <td>0006428320</td>\n",
        "      <td>3.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>A3TS466QBAWB9D</td>\n",
        "      <td>0014072149</td>\n",
        "      <td>5.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>A3BUDYITWUSIS7</td>\n",
        "      <td>0041291905</td>\n",
        "      <td>5.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>A19K10Z0D2NTZK</td>\n",
        "      <td>0041913574</td>\n",
        "      <td>5.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>A14X336IB4JD89</td>\n",
        "      <td>0201891859</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td>A2HR0IL3TC4CKL</td>\n",
        "      <td>0577088726</td>\n",
        "      <td>5.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>A2DHYD72O52WS5</td>\n",
        "      <td>0634029231</td>\n",
        "      <td>3.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>A1MUVHT8BONL5K</td>\n",
        "      <td>0634029347</td>\n",
        "      <td>2.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td>A15GZQZWKG6KZM</td>\n",
        "      <td>0634029347</td>\n",
        "      <td>1.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td>A16WE7UU0QD33D</td>\n",
        "      <td>0634029347</td>\n",
        "      <td>5.0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "             user        item  rating\n",
        "0  A1YS9MDZP93857  0006428320     3.0\n",
        "1  A3TS466QBAWB9D  0014072149     5.0\n",
        "2  A3BUDYITWUSIS7  0041291905     5.0\n",
        "3  A19K10Z0D2NTZK  0041913574     5.0\n",
        "4  A14X336IB4JD89  0201891859     1.0\n",
        "5  A2HR0IL3TC4CKL  0577088726     5.0\n",
        "6  A2DHYD72O52WS5  0634029231     3.0\n",
        "7  A1MUVHT8BONL5K  0634029347     2.0\n",
        "8  A15GZQZWKG6KZM  0634029347     1.0\n",
        "9  A16WE7UU0QD33D  0634029347     5.0"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### We set K, the latent dimension of the kernel matrix $L$ to be 50. This gives a restriction on the maximum number of items a user's basket can contain. This restriction do not discard to many users in this case."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "K=50"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### We artificially densify the matrix so that we can get some results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_item_count = data.groupby(\"item\")[\"user\"].count().reset_index()\n",
      "popular_items = data_item_count[data_item_count[\"user\"]>20]\n",
      "del popular_items[\"user\"]\n",
      "dense_item_data = pd.merge(data, popular_items, on=['item'])\n",
      "data_user_count = dense_item_data.groupby(\"user\")[\"item\"].count().reset_index()\n",
      "popular_users = data_user_count[np.logical_and(data_user_count[\"item\"] >5,data_user_count[\"item\"] < K) ]\n",
      "del popular_users[\"item\"]\n",
      "dense_data = pd.merge(dense_item_data, popular_users, on=['user'])\n",
      "users = dense_data['user'].unique()\n",
      "items = dense_data['item'].unique()\n",
      "ratings = dense_data['rating'].unique()\n",
      "U = len(users)\n",
      "M = len(items)\n",
      "users_dict = pd.DataFrame(range(U), users).to_dict()[0]\n",
      "items_dict = pd.DataFrame(range(M), items).to_dict()[0]\n",
      "dense_data['user'] = dense_data['user'].apply(lambda x: users_dict[x])\n",
      "dense_data['item'] = dense_data['item'].apply(lambda x: items_dict[x])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### We want to learn how to find the rating of any item given the user"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = dense_data[[\"user\", \"item\"]].values\n",
      "y = dense_data[\"rating\"].values\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Length of test set: %i\"%len(X_test))\n",
      "print(\"Matrix sparsity: %f\"%(len(X_train)/float(U*M)))\n",
      "print(\"Total number of users: %i\"%U)\n",
      "print(\"Total number of items: %i\"%M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Length of test set: 151\n",
        "Matrix sparsity: 0.002861\n",
        "Total number of users: 1761\n",
        "Total number of items: 2963\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training\n",
      "We maximize the penalized log-likelihood using accelerated stochastic gradient ascent."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train(X_train):\n",
      "    t_data = pd.DataFrame(X_train,columns= [\"user\", \"item\"])\n",
      "    t_users = t_data[\"user\"].unique()\n",
      "    t_items = t_data[\"item\"].unique()\n",
      "    t_gb_item = t_data.groupby('item')['user'].apply(list)\n",
      "    t_dict_item = t_gb_item.to_dict()\n",
      "    t_gb_users = t_data.groupby('user')['item'].apply(list)\n",
      "    t_dict_users = t_gb_users.to_dict()\n",
      "\n",
      "    #Computing items popularity penalisation (the more it is popular the less it is penalized)\n",
      "\n",
      "    popularity = {}\n",
      "    for k,v in t_dict_item.items():\n",
      "        popularity[k] = 1.0/len(v)\n",
      "\n",
      "    # Computing the penalized log-likelyhood\n",
      "    def f(V,a):\n",
      "        def V_(n):\n",
      "            return V[np.array(t_dict_users[n])]\n",
      "\n",
      "        # This determinant is very unlikely to be zero if K is properly set. It is indeed very unlikely\n",
      "        # that for a random matrix of rank K that there exist $n < K$ rows that are not independant.\n",
      "        res = 0\n",
      "        for n in t_users:\n",
      "            det = np.linalg.det(V_(n).dot(V_(n).T))\n",
      "            if det <= 0:\n",
      "                print(\"Unlucky you, the determinant was negative or null\")\n",
      "\n",
      "            res += np.log(det)\n",
      "\n",
      "        \n",
      "        pen = 0\n",
      "        for i in t_dict_item.keys():\n",
      "            pen += popularity[i]*V[i,:].dot(V[i,:].T)\n",
      "\n",
      "        return res - len(t_dict_item)*np.log(np.linalg.det(V.dot(V.T) + np.identity(M))) - a/2*pen\n",
      "\n",
      "    # Computing the gradient of the penalized log-likelihood\n",
      "    def grad_l(V,a):\n",
      "\n",
      "        def V_(n):\n",
      "            return V[np.array(t_dict_users[n])]\n",
      "\n",
      "        # We compute un advance the inverse matrix we will need \n",
      "        l_dict_ind = {}\n",
      "        A = {}\n",
      "        for n in t_users:\n",
      "            dict_ind = {}\n",
      "            V_n = V[np.array(t_dict_users[n])]\n",
      "            A[n] = np.linalg.inv(V_n.dot(V_n.T))\n",
      "            for i in range(len(t_dict_users[n])):\n",
      "                dict_ind[t_dict_users[n][i]] = i\n",
      "            l_dict_ind[n] = dict_ind\n",
      "                \n",
      "        B = np.identity(M) - V.dot(np.linalg.inv(np.identity(K) + (V.T).dot(V))).dot(V.T)\n",
      "\n",
      "        # We compute the gradient\n",
      "        G = np.zeros((M,K))\n",
      "        for i in t_dict_item.keys():\n",
      "            for k in range(K):\n",
      "                grad = 0\n",
      "                for n in t_dict_item[i]:\n",
      "                    i_ind = l_dict_ind[n][i]\n",
      "                    grad += A[n][i_ind,:].dot(V_(n)[:,k]) + A[n][:,i_ind].dot(V_(n)[:,k])\n",
      "                \n",
      "                if np.isinf(grad) or np.isnan(grad):\n",
      "                     print(\"Pb\")\n",
      "                        \n",
      "                G[i,k] = grad - len(t_users)*(B[i,:].dot(V[:,k]) + B[:,i].dot(V[:,k])) - a*popularity[i]*V[i,k]\n",
      "        \n",
      "        return G    \n",
      "\n",
      "\n",
      "    # Optimisation scheme: accelerated stochastic gradient descent\n",
      "    def SGD(b, e0,a):\n",
      "        V = np.random.rand(M,K)*10\n",
      "        W = np.zeros((M,K))\n",
      "        val = f(V,a)\n",
      "        for t in range(0,1000000):\n",
      "            e = e0\n",
      "            W = W*b + (1-b)*e*grad_l(V + b*W,a)\n",
      "            V = V + W\n",
      "            \n",
      "            # This condition set a value from where to stop (this is more or less \n",
      "            # what we would do intuitively: when the train error goes up we stop)\n",
      "            if f(V,a) > val:\n",
      "                val = f(V,a)\n",
      "            else:\n",
      "                break\n",
      "\n",
      "        return val, V\n",
      "\n",
      "    # We compute the optimum\n",
      "    f_V, V = SGD(0.9, 100,0.01)\n",
      "    \n",
      "    # We use the optimum obtained before to compute the prediction function\n",
      "    def predict(test_user, V):\n",
      "        # If the user is known we can use the method to do the prediction\n",
      "        if test_user in t_dict_users:\n",
      "            \n",
      "            # We compute L_A\n",
      "            A = np.array(t_dict_users[test_user])\n",
      "            all_users = np.sort(items_dict.values())\n",
      "            A_bar = np.delete(all_users, A)\n",
      "            Z_A = np.identity(K) - (V[A].T).dot(np.linalg.inv(V[A].dot(V[A].T))).dot(V[A])\n",
      "            V_A = V[A_bar].dot(Z_A)\n",
      "            L_A = V_A.dot(V_A.T)\n",
      "\n",
      "            # We compute the probability to buy a new item i. If i is in A this probability is 0. Otherwise it is given by L_A\n",
      "            # (indexed by A_bar)\n",
      "            res = []\n",
      "            for i in range(M):\n",
      "                found = 0\n",
      "                for i_b in range(len(A_bar)):\n",
      "                    if A_bar[i_b] == i:\n",
      "                        found = 1\n",
      "                        res.append(L_A[i_b,i_b])\n",
      "                if found == 0:\n",
      "                    res.append(0)\n",
      "\n",
      "            res = np.array(res)\n",
      "\n",
      "            return res/res.sum()\n",
      "        else:\n",
      "            # If we have never seen the user, a reasonnable assumption is uniform probability.\n",
      "            return np.ones(M)/float(M)\n",
      "\n",
      "    return predict, V"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Start learning\")\n",
      "X1 = []\n",
      "X2 = []\n",
      "X3 = []\n",
      "X4 = []\n",
      "X5 = []\n",
      "\n",
      "#We separate our training data in 5 sets according to the ratings.\n",
      "\n",
      "for i in range(len(X_train)):\n",
      "    if y[i] == 1:\n",
      "        X1.append(X_train[i])\n",
      "    if y[i] == 2:\n",
      "        X2.append(X_train[i])\n",
      "    if y[i] == 3:\n",
      "        X3.append(X_train[i])\n",
      "    if y[i] == 4:\n",
      "        X4.append(X_train[i])\n",
      "    if y[i] == 5:\n",
      "        X5.append(X_train[i])\n",
      "\n",
      "print(\"List made\")\n",
      "X_train_list = [X1, X2, X3, X4, X5]\n",
      "\n",
      "#We learn our model on these 5 sets\n",
      "print(\"Learning\")\n",
      "c1 = 0\n",
      "y_predict = []\n",
      "predict_list = []\n",
      "for X_ in X_train_list:\n",
      "    c1 += 1\n",
      "    print(\"Learning %i/100\"%(c1*100./len(X_train_list)))\n",
      "    predict, V = train(X_)\n",
      "    predict_list.append(predict)\n",
      "\n",
      "# The item that is given the highest probability between all five test sets gets selected.\n",
      "print(\"Predicting\")\n",
      "c2=0\n",
      "for u_test, i_test in X_test:\n",
      "    c2 += 1\n",
      "    if (c2*100/len(X_test) % 20) == 0:\n",
      "        print(\"Predicting %i/100\"%(c2*100/len(X_test)))\n",
      "    \n",
      "    res_i = []\n",
      "    for pred in predict_list:\n",
      "        res = pred(u_test,V)\n",
      "        res_i.append(res[i_test])\n",
      "    y_predict.append(np.argmax(np.array(res_i))+1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Start learning\n",
        "List made\n",
        "Learning\n",
        "Learning 20/100\n",
        "Learning 40/100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Learning 60/100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Learning 80/100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Learning 100/100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Predicting"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Predicting 0/100\n",
        "Predicting 20/100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Predicting 40/100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Predicting 60/100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Predicting 60/100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Predicting 80/100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Predicting 80/100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Predicting 100/100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We compute the mean square error\n",
      "err = y_predict - y_test\n",
      "sqrm_err = err.dot(err)/len(err)\n",
      "print(sqrm_err)\n",
      "print(y_predict)\n",
      "print(y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6.2582781457\n",
        "[3 2 1 5 3 1 5 2 5 1 5 3 2 2 5 1 1 3 5 2 3 4 1 5 1 3 5 1 1 1 2 4 3 5 4 5 5\n",
        " 1 2 5 3 5 3 3 5 3 5 2 1 5 3 2 1 2 1 1 1 2 1 1 5 5 1 5 2 2 4 3 1 5 1 5 1 5\n",
        " 1 1 2 2 3 3 4 1 1 5 5 2 5 5 3 1 1 1 2 5 5 5 3 5 3 4 2 4 1 1 5 5 2 1 1 1 1\n",
        " 5 5 2 1 1 5 5 1 1 5 5 3 3 5 2 3 1 5 4 5 1 4 1 1 3 1 1 2 3 2 4 1 1 1 5 4 5\n",
        " 1 5 3]\n",
        "[ 5.  3.  5.  5.  5.  4.  4.  5.  5.  5.  5.  5.  5.  5.  4.  5.  5.  5.\n",
        "  2.  5.  4.  4.  5.  5.  5.  5.  4.  5.  5.  4.  3.  1.  5.  4.  3.  5.\n",
        "  4.  5.  1.  4.  5.  5.  5.  5.  4.  5.  5.  5.  5.  4.  5.  2.  5.  5.\n",
        "  3.  5.  4.  3.  5.  4.  3.  5.  3.  5.  5.  5.  5.  5.  2.  5.  5.  5.\n",
        "  3.  5.  5.  5.  5.  5.  4.  3.  2.  4.  5.  5.  5.  2.  4.  5.  4.  5.\n",
        "  2.  5.  5.  5.  4.  5.  5.  1.  2.  5.  5.  5.  4.  4.  3.  5.  5.  5.\n",
        "  5.  5.  5.  5.  4.  5.  5.  4.  5.  5.  3.  5.  5.  3.  5.  5.  1.  5.\n",
        "  5.  4.  5.  4.  5.  5.  3.  5.  5.  5.  5.  5.  5.  4.  5.  5.  5.  5.\n",
        "  1.  5.  5.  5.  5.  5.  1.]\n"
       ]
      }
     ],
     "prompt_number": 193
    }
   ],
   "metadata": {}
  }
 ]
}